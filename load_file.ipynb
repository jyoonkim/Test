{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from no_PII_Storage_folder import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUM_FILES = 8\n",
    "# COURSE_NUM = '6002x_'\n",
    "path_to_files = 'data/clickstream/'\n",
    "\n",
    "NUM_FILES = 32\n",
    "COURSE_NUM = '6001x_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['6001x_9__2T2016', '6001x__2T2016', '6002x_5__1T2016', '6002x_6__3T2016', '6002x_7__1T2017', 'csv', 'sample']\n",
      "['6001x_19', '6001x_39', '6001x_00', '6001x_01', '6001x_02', '6001x_03', '6001x_04', '6001x_05', '6001x_06', '6001x_07', '6001x_08', '6001x_09', '6001x_10', '6001x_11', '6001x_12', '6001x_13', '6001x_14', '6001x_15', '6001x_16', '6001x_17', '6001x_18', '6001x_20', '6001x_21', '6001x_22', '6001x_23', '6001x_24', '6001x_25', '6001x_26', '6001x_27', '6001x_28', '6001x_29', '6001x_30', '6001x_31', '6001x_32', '6001x_33', '6001x_34', '6001x_35', '6001x_36', '6001x_37', '6001x_38', '6001x_40', '6001x_41', '6001x_42', '6001x_43', '6001x_44', '6001x_45', '6001x_46', '6001x_47', '6001x_48', '6001x_49', '6001x_50', '6001x_51', '6001x_52', '6001x_53', '6001x_54', '6001x_55', '6001x_56', '6001x_57', '6001x_58', '6001x_59', '6001x_60', '6001x_61', '6001x_62', '6001x_63', '6001x_64', '6001x_65']\n"
     ]
    }
   ],
   "source": [
    "all_dir = os.listdir(path_to_files)\n",
    "print (all_dir)\n",
    "single_dir = os.listdir(path_to_files + '/6001x__2T2016')\n",
    "print (single_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to load in 60+ GB files, so we wil try loading in the first data in all_dir into chunks of 8 million, then later in the notebook files we can read them all in and concatenate them after they've decreased in size\n",
    "<br>***How to do this?***\n",
    "<br>-add to_csv to loop AFTER 8 json files are read and cleaned "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_data = pd.DataFrame()\n",
    "\n",
    "# #for count in range(0,2):\n",
    "#     single_dir = os.listdir(path_to_files + all_dir[count])\n",
    "#     print (single_dir)\n",
    "#     for num in range(len(single_dir)):\n",
    "#         if num < 10:\n",
    "#             user_data_org = pd.read_json(path_to_files + all_dir[count] + '/6001x_0' + str(num),  lines = True)\n",
    "#         else:\n",
    "#             user_data_org = pd.read_json(path_to_files + all_dir[count] + '/6001x_' + str(num),  lines = True)\n",
    "#         user_data_org = user_data_org.drop(['context', 'course_id', 'host', 'event_struct', 'event_type', 'ip', 'page', 'mongoid', 'session','module_id', 'event'], axis = 'columns')\n",
    "#         user_data_org['username'] = hash_column(user_data_org['username'])\n",
    "#         user_data = pd.concat([user_data_org, user_data], axis = 0, sort = False)\n",
    "#         print(count, len(user_data))\n",
    "\n",
    "        \n",
    "#ERROR: COULD NOT RESERVE MEMORY BLOCK--HOW TO READ JSON FILES WHILE USING LESS MEMORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count = 1\n",
      "count = 2\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Could not reserve memory block",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-34dadb17e8e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'count ='\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0muser_data_org\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_files\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mall_dir\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/6001x_0'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0muser_data_org\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_files\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mall_dir\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/6001x_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, lines, chunksize, compression)\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 592\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    593\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshould_close\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    713\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 715\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_combine_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_get_object_parser\u001b[0;34m(self, json)\u001b[0m\n\u001b[1;32m    737\u001b[0m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"frame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFrameParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"series\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 849\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_no_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    851\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_parse_no_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1091\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m             self.obj = DataFrame(\n\u001b[0;32m-> 1093\u001b[0;31m                 \u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecise_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecise_float\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1094\u001b[0m             )\n\u001b[1;32m   1095\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"split\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Could not reserve memory block"
     ]
    }
   ],
   "source": [
    "user_data = pd.read_json(path_to_files + all_dir[1] + '/6001x_00',  lines = True)\n",
    "user_data = user_data.drop(['context', 'course_id', 'host', 'event_struct', 'event_type', 'ip', 'page', 'mongoid', 'session','module_id', 'event'], axis = 'columns')\n",
    "user_data['username'] = hash_column(user_data['username'])\n",
    "num = 5\n",
    "\n",
    "#data/clickstream/6001x_9__2T2016/6001x_00\n",
    "\n",
    "for count in range(1, len(single_dir)):\n",
    "    print('count =', count)\n",
    "    if count < 10:\n",
    "        user_data_org = pd.read_json(path_to_files + all_dir[1] + '/6001x_0' + str(count),  lines = True)\n",
    "    else:\n",
    "        user_data_org = pd.read_json(path_to_files + all_dir[1] + '/6001x_' + str(count),  lines = True)\n",
    "    user_data_org = user_data_org.drop(['context', 'course_id', 'host', 'event_struct', 'event_type', 'ip', 'page', 'mongoid', 'session','module_id', 'event'], axis = 'columns')\n",
    "    user_data_org['username'] = hash_column(user_data_org['username'])\n",
    "   # tmp = user_data_org\n",
    "    user_data = pd.concat([user_data_org, user_data], axis = 0, sort = False)\n",
    "    if count % 7 == 0:\n",
    "        user_data.to_csv('user_data_6001x_'+ str(num), header = True)\n",
    "        print(str(num) + ' :) ' + str(len(user_data)))\n",
    "        num += 1\n",
    "        user_data = user_data_org\n",
    "        \n",
    "user_data.to_csv('user_data_6001x_'+ str(num), header = True)\n",
    "print(str(num) + ' :) ' + str(len(user_data)))\n",
    "num += 1\n",
    "\n",
    "#went to 7 / 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27000000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(user_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports data to csv file, if working with new data, consider writing it to new csv file\n",
    "\n",
    "# user_data.to_csv(r'\\ALL_DATA.csv', header = True)\n",
    "user_data.to_csv(r'user_data_6001x', header = true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data_0 = pd.read_csv('user_data_6001x_0', dtype={\"time\": str, \"username\": str, \"event_source\": str, \"agent\": str, \"name\": str})\n",
    "user_data_1 = pd.read_csv('user_data_6001x_1', dtype={\"time\": str, \"username\": str, \"event_source\": str, \"agent\": str, \"name\": str})\n",
    "user_data_2 = pd.read_csv('user_data_6001x_2', dtype={\"time\": str, \"username\": str, \"event_source\": str, \"agent\": str, \"name\": str})\n",
    "user_data_3 = pd.read_csv('user_data_6001x_3', dtype={\"time\": str, \"username\": str, \"event_source\": str, \"agent\": str, \"name\": str})\n",
    "user_data_4 = pd.read_csv('user_data_6001x_4', dtype={\"time\": str, \"username\": str, \"event_source\": str, \"agent\": str, \"name\": str})\n",
    "user_data_5 = pd.read_csv('user_data_6001x_5', dtype={\"time\": str, \"username\": str, \"event_source\": str, \"agent\": str, \"name\": str})\n",
    "user_data_6 = pd.read_csv('user_data_6001x_6', dtype={\"time\": str, \"username\": str, \"event_source\": str, \"agent\": str, \"name\": str})\n",
    "user_data_7 = pd.read_csv('user_data_6001x_7', dtype={\"time\": str, \"username\": str, \"event_source\": str, \"agent\": str, \"name\": str})\n",
    "\n",
    "\n",
    "user_data = pd.concat([user_data_0, user_data_1, user_data_2, user_data_3, user_data_4, user_data_5, user_data_6, user_data_7], axis = 0, sort = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "There are 984109 user-agents that contain: Mobi or mobi\n",
      "\n",
      "There are 58328665 user-agents that DON'T contain:  Mobi or mobi\n"
     ]
    }
   ],
   "source": [
    "MOBILE_INDICATOR_1 = \"mobi\"\n",
    "MOBILE_INDICATOR_2 = \"Mobi\"\n",
    "\n",
    "mobile_data = pd.DataFrame()\n",
    "non_mobile_data = pd.DataFrame()\n",
    "\n",
    "mobile_data = user_data[user_data['agent'].str.contains(MOBILE_INDICATOR_1, na = False) | user_data['agent'].str.contains(MOBILE_INDICATOR_2, na = False)] \n",
    "non_mobile_data = user_data[~user_data['agent'].str.contains(MOBILE_INDICATOR_1, na = False) &  ~user_data['agent'].str.contains(MOBILE_INDICATOR_2, na = False)] \n",
    "\n",
    "mobile_data = mobile_data.reset_index(drop=True)\n",
    "non_mobile_data = non_mobile_data.reset_index(drop=True)\n",
    "    \n",
    "print(\"\\nThere are \" + str(len(mobile_data)) + \" user-agents that contain: Mobi or mobi\")\n",
    "print(\"\\nThere are \" + str(len(non_mobile_data)) + \" user-agents that DON'T contain:  Mobi or mobi\")\n",
    "\n",
    "# Mobi ref : https://delib.zendesk.com/hc/en-us/articles/203431259-Browser-device-identification-how-to-find-out-which-browser-and-device-have-been-used-to-submit-a-response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>time</th>\n",
       "      <th>username</th>\n",
       "      <th>event_source</th>\n",
       "      <th>agent</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2016-06-16 15:36:11.04722 UTC</td>\n",
       "      <td>5bd68e3d5fb69e16ad13df9379d37d6e</td>\n",
       "      <td>browser</td>\n",
       "      <td>Mozilla/5.0 (Linux; Android 5.1; Z958 Build/LM...</td>\n",
       "      <td>edx.ui.lms.outline.selected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>2016-06-16 07:15:05.292872 UTC</td>\n",
       "      <td>fde1688af200c48b57920fcdc9daa71b</td>\n",
       "      <td>server</td>\n",
       "      <td>Mozilla/5.0 (Linux; Android 5.1.1; Redmi Note ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>67</td>\n",
       "      <td>2016-06-16 11:29:26.267787 UTC</td>\n",
       "      <td>4c1c393df3f2354a6cbd7f55056c41df</td>\n",
       "      <td>browser</td>\n",
       "      <td>Mozilla/5.0 (Linux; Android 5.0; SM-N900 Build...</td>\n",
       "      <td>edx.video.closed_captions.hidden</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                            time  \\\n",
       "0           8   2016-06-16 15:36:11.04722 UTC   \n",
       "1          27  2016-06-16 07:15:05.292872 UTC   \n",
       "2          67  2016-06-16 11:29:26.267787 UTC   \n",
       "\n",
       "                           username event_source  \\\n",
       "0  5bd68e3d5fb69e16ad13df9379d37d6e      browser   \n",
       "1  fde1688af200c48b57920fcdc9daa71b       server   \n",
       "2  4c1c393df3f2354a6cbd7f55056c41df      browser   \n",
       "\n",
       "                                               agent  \\\n",
       "0  Mozilla/5.0 (Linux; Android 5.1; Z958 Build/LM...   \n",
       "1  Mozilla/5.0 (Linux; Android 5.1.1; Redmi Note ...   \n",
       "2  Mozilla/5.0 (Linux; Android 5.0; SM-N900 Build...   \n",
       "\n",
       "                               name  \n",
       "0       edx.ui.lms.outline.selected  \n",
       "1                               NaN  \n",
       "2  edx.video.closed_captions.hidden  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mobile_data.head(n = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mobile_data.to_csv(r'MOBILE_DATA.csv', header = True)\n",
    "# non_mobile_data.to_csv(r'NON_MOBILE_DATA.csv', header = True)\n",
    "\n",
    "mobile_data = mobile_data.dropna()\n",
    "mobile_data = mobile_data.reset_index(drop=True)\n",
    "non_mobile_data = non_mobile_data.dropna()\n",
    "non_mobile_data = non_mobile_data.reset_index(drop=True)\n",
    "\n",
    "mobile_data.to_csv(r'MOBILE_DATA_01.csv', header = True)\n",
    "non_mobile_data.to_csv(r'NON_MOBILE_DATA_01.csv', header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
